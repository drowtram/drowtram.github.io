---
layout:     post
title:      数据结构与算法之美学习笔记
subtitle:   基础篇之排序优化
date:       2019-10-24
author:     Alee
header-img: img/head/niceSort.jpg
catalog: true
tags:
    - 数据结构
    - 算法
    - 排序
    - 排序优化
---

## 基础篇之排序优化

> 你知道如何实现一个通用的、高性能的排序函数？



在平时的开发中，我们经常直接使用平台编程语言所提供的排序函数，比如Java的Arrays.sort()方法，你知道这些排序函数是如何实现的吗？底层都利用了哪种排序算法呢？



#### 如何选择合适的排序算法？

要实现一个通用的、高效率的排序函数，该如何选择排序算法？

![pxsf](https://static001.geekbang.org/resource/image/1f/fd/1f6ef7e0a5365d6e9d68f0ccc71755fd.jpg)

由于线性排序的时间复杂度比较低，适用场景比较特殊。所以要写一个通用的排序函数，不能选择线性排序算法。

如果对小规模数据进行排序，可以选择时间复杂度是O(n²)的算法；如果对大规模数据进行排序，时间复杂度是O(n㏒n)的算法更加高效。所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是O(n㏒n)的排序算法来实现排序函数。

时间复杂度是O(n㏒n)的排序算法有归并排序、快速排序，还有堆排序。堆排序和快速排序都有比较多的应用，比如Java语言采用堆排序实现排序函数，C语言使用快速排序实现排序函数。

使用归并排序的情况并不多。快排在最坏情况下的时间复杂度是O(n²)，而归并排序可以做到平均情况、最坏情况下的时间复杂度都是O(n㏒n)，看起来挺好，但是归并排序不是原地排序算法，空间复杂度是O(n)。如果要排序100MB的数据 ，除了数据本身占用的内存之外，排序算法还要额外再占用100MB的内存空间，空间耗费就翻倍了。

快速排序比较适合来实现排序函数，但是，快排在最坏情况下的时间复杂度是O(n²)。我们得解决这个“复杂度恶化”的问题。



#### 如何优化快速排序？

如果数据原来就是有序的或者接近有序，每次分区点都选择最后一个数据，那快速排序算法就会变得非常糟糕，时间复杂度就退化为O(n²)。**这种O(n²)时间复杂度出现的主要原因还是因为我们分区点选的不够合理**。

那如何来选择分区点呢？

最理想的分区点是：**被分区点分开的两个分区中，数据的数量差不多。**

如果粗暴地直接选择第一个或者最后一个数据作为分区点，不考虑数据的特点，在某些情况下，排序的最坏情况时间复杂度就是O(n²)。为了提高排序算法的性能，我们要尽可能地让每次分区都比较平均。

怎么尽可能地做到分区比较平均？这里有两种比较常用、比较简单的分区算法。

1. **三数取中法**

   从区间的首、尾、中间，分别取出一个数，然后对比大小，取这3个数的中间值作为分区点。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。

2. **随机法**

   随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选得比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选得很差的情况，所以平均情况下，这样选的分区点是比较好的。时间复杂度退化为O(n²)的情况，出现的可能性不大。

快排是用递归来实现的，在学递归的时候知道，递归要警惕堆栈溢出。为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，我们有两种解决办法：第一种是限制递归深度。一旦递归过深，超过我们事先设计的阈值，就停止递归。第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。